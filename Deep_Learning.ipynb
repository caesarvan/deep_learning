{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y2osEUslUGkv"
   },
   "source": [
    "# EE5934 Deep Learning Project 1\n",
    "\n",
    "In this project, you are going to explore Deep Learning and Neural Networks by completing the following three tasks:\n",
    "\n",
    "* Task 1. Network Exploration (30%)\n",
    "* Task 2. Model Interpretation (30%)\n",
    "* Task 3. Adversarial Attack (40%)\n",
    "\n",
    "Before doing the project, please read the instructions carefully (failure to do so will be penalized):\n",
    "\n",
    "1. Implement your codes **within** \"TODO\" and \"END OF YOUR CODE\", do **NOT** modify any codes outside the answer area;\n",
    "2. Make sure your codes **clean**, **easily readable** (add meaningful comments if needed), and **runnable**;\n",
    "3. Write your answers in the given markdown cells, keep your answers clear and concise;\n",
    "4. Do submit your project to \"Files/Project 1/Submissions\" on [LumiNUS](https://luminus.nus.edu.sg/) before the deadline: **5:59 pm (SGT), 6 March, 2022**;\n",
    "5. This is an individual project, do **NOT** share your solutions with others, we have zero tolerance for cheating.\n",
    "\n",
    "If you have any questions regarding this project, please feel free to contact Wu Zhangjie (zhangjiewu@u.nus.edu). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyJICDR5tkuj"
   },
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OmjVbh2pHxd6"
   },
   "source": [
    "### Loading packages\n",
    "\n",
    "Please install the packages listed below if you haven't done so. To avoid unnecessary trouble while reproducing your code, please install the required [PyTorch](https://pytorch.org/) version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "LNFcTvFWg1L1"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Please reinstall pytorch to 1.9.0.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2273339450a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'1.9.0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Please reinstall pytorch to 1.9.0.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'0.10.0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Please reinstall torchvision to 0.10.0.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Please reinstall pytorch to 1.9.0."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.cm as mpl_color_map\n",
    "from PIL import Image\n",
    "from random import randint\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "# assert torch.__version__ == '1.9.0', 'Please reinstall pytorch to 1.9.0.'\n",
    "# assert torchvision.__version__ == '0.10.0', 'Please reinstall torchvision to 0.10.0.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "b9Y1SvTSM-Dx",
    "outputId": "40f30d87-413c-45c8-8f79-eb4a7b198111"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0+cu102\n",
      "0.10.0+cu102\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Owj6jKvWraFJ"
   },
   "source": [
    "### Checking the Running Device\n",
    "\n",
    "It is recommended to run this notebook on GPUs since that would be a much faster way to train and evaluate the model. If you have difficulties accessing GPUs with your machine, you can choose to use the free GPUs provided by [Google Colab](https://colab.research.google.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CLovzR7Lk49o",
    "outputId": "c04a56bb-2f7d-4e5d-bd70-659c059696e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = 'cuda:0'\n",
    "else:\n",
    "  device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fb12tK49rDia"
   },
   "source": [
    "### Getting the CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EIT2XbCfp2iv",
    "outputId": "002a3774-a6c9-4565-8976-92317ad761d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: data/cifar-10-python.tar.gz\n",
      "Extracting data/cifar-10-python.tar.gz to data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "CIFAR_CLASSES = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
    "CIFAR_MEAN, CIFAR_STD = np.array([0.4914, 0.4822, 0.4465]), np.array([0.247, 0.243, 0.261])\n",
    "\n",
    "# Convert image to pytroch tensor and normalize\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=CIFAR_MEAN.tolist(), std=CIFAR_STD.tolist())\n",
    "])\n",
    "\n",
    "# Inverse operation to regain original image\n",
    "inverse_transform = torchvision.transforms.Compose([ \n",
    "    torchvision.transforms.Normalize(mean=[0, 0, 0], std=(1 / CIFAR_STD).tolist()),\n",
    "    torchvision.transforms.Normalize(mean=(-CIFAR_MEAN).tolist(), std=[1, 1, 1]),\n",
    "    torchvision.transforms.ToPILImage()\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "train_set = torchvision.datasets.CIFAR10(root=\"data\", train=True, download=True, transform=transform)\n",
    "train_size = len(train_set)\n",
    "test_set = torchvision.datasets.CIFAR10(root=\"data\", train=False, download=True, transform=transform)\n",
    "test_size = len(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGK-gPAIHxeD"
   },
   "source": [
    "### Helper Fuctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "nZnOSvFFHxeE"
   },
   "outputs": [],
   "source": [
    "def show_prob_cifar(image, label, p):\n",
    "    \"\"\"\n",
    "        Show image and prediction probability for CIFAR-10 dataset.\n",
    "    Args:\n",
    "    Inputs\n",
    "        image (Tensor): Input image\n",
    "        label (int): The ground truth label of input image\n",
    "        p (Tensor): Class probability of input image\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    ft = 10\n",
    "    width = 0.9\n",
    "    col = 'blue'\n",
    "\n",
    "    p=p.cpu().data.squeeze().numpy()\n",
    "    y_pos = np.arange(len(p))\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "    # Plot image\n",
    "    ax1 = plt.subplot(2, 1, 1)\n",
    "    ax1.imshow(image)\n",
    "    ax1.set_axis_off()\n",
    "    ax1.set_title('Ground Truth: ' + CIFAR_CLASSES[label])\n",
    "\n",
    "    # Plot probability\n",
    "    ax2 = plt.subplot(2, 1, 2)\n",
    "    ax2.barh(y_pos, p*0.1, width , align='center', color=col)\n",
    "    ax2.set_yticks(y_pos)\n",
    "    ax2.set_yticklabels(CIFAR_CLASSES, fontsize=ft)\n",
    "    ax2.invert_yaxis()  \n",
    "    ax2.set_xticklabels([])\n",
    "    ax2.set_xticks([])\n",
    "    ax2.spines['right'].set_visible(False)\n",
    "    ax2.spines['top'].set_visible(False)\n",
    "    ax2.spines['bottom'].set_visible(False)\n",
    "    ax2.spines['left'].set_linewidth(4)\n",
    "    for i in range(len(p)):\n",
    "        str_nb=\"{0:.2f}\".format(p[i])\n",
    "        ax2.text(p[i]*0.1 + 0.001, y_pos[i] ,str_nb ,\n",
    "                 horizontalalignment='left', verticalalignment='center',\n",
    "                 transform=ax2.transData, color= col,fontsize=ft)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def apply_colormap_on_image(org_im, activation, colormap_name):\n",
    "    \"\"\"\n",
    "        Apply heatmap on image\n",
    "    Args:\n",
    "        org_img (PIL img): Original image\n",
    "        activation_map (numpy arr): Activation map (grayscale) 0-255\n",
    "        colormap_name (str): Name of the colormap\n",
    "    \"\"\"\n",
    "    # Get colormap\n",
    "    color_map = mpl_color_map.get_cmap(colormap_name)\n",
    "    no_trans_heatmap = color_map(activation)\n",
    "    # Change alpha channel in colormap to make sure original image is displayed\n",
    "    heatmap = copy.copy(no_trans_heatmap)\n",
    "    heatmap[:, :, 3] = 0.4\n",
    "    heatmap = Image.fromarray((heatmap*255).astype(np.uint8))\n",
    "    no_trans_heatmap = Image.fromarray((no_trans_heatmap*255).astype(np.uint8))\n",
    "\n",
    "    # Apply heatmap on iamge\n",
    "    heatmap_on_image = Image.new(\"RGBA\", org_im.size)\n",
    "    heatmap_on_image = Image.alpha_composite(heatmap_on_image, org_im.convert('RGBA'))\n",
    "    heatmap_on_image = Image.alpha_composite(heatmap_on_image, heatmap)\n",
    "    return no_trans_heatmap, heatmap_on_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UzXv8qM1rPlc"
   },
   "source": [
    "## Task 1: Network Exploration\n",
    "\n",
    "In this task, you are expected to build a convolutional neural network (CNN) for solving a problem of image recognition and explore strategies that could further strengthen its performance.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NElByUjHxeH"
   },
   "source": [
    "### Defining the Model\n",
    "\n",
    "In deep learning, a Convolutional Neural Network (CNN, or ConvNet) is one of the most famous deep learning models that have been widely used in the field of computer vision. A simple CNN often consists of three main types of layers:\n",
    "- **Convolutional Layer**, the core building block that takes over the most of computational burdens, contains a set of filters (or kernels) with learnable parameters while training. It receives as input an image (or a feature map), and computes its output volume by stacking the activation maps convolved by every filter along the depth dimension. \n",
    "- **Pooling Layer** is commonly inserted in-between successive Conv layers in a CNN architecture. Its function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network, and hence to also control overfitting. \n",
    "- **Fully Connected Layer** have fully connections to all activations in the previous layer and is usually placed before the output layer to form the last few layers of a CNN Architecture.\n",
    "\n",
    "If you are not familiar with CNN architectures, [this blog](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53) may help you get a better understanding of the mechanism of each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nX61R0I8HxeI"
   },
   "source": [
    "Define a naive CNN stacked by the layers mentioned above. While doing so, please follow the TODOs provided below. (Refer to [torch.nn](https://pytorch.org/docs/stable/nn.html) for detailed instructions of building basic blocks with PyTorch.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LVv2SxOihl6K",
    "outputId": "a41e72c5-41fc-46fc-9423-0af0f91986f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (fc1): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (fc3): Linear(in_features=4096, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        ##############################################################################\n",
    "        # TODO: Define a simple CNN contraining Conv, Pooling, and FC layers.        #\n",
    "        ##############################################################################\n",
    "        \n",
    "        # Block 1:         3 x 32 x 32 --> 64 x 16 x 16   \n",
    "        self.conv1 = torch.nn.Conv2d(3, 64, kernel_size=3, padding=1, stride=1)\n",
    "\n",
    "        # Block 2:         64 x 16 x 16 --> 128 x 8 x 8\n",
    "        self.conv2 = torch.nn.Conv2d(64, 128, kernel_size=3, padding=1, stride=1)\n",
    "\n",
    "        # Block 3:         128 x 8 x 8 --> 256 x 4 x 4        \n",
    "        self.conv3 = torch.nn.Conv2d(128, 256, kernel_size=3, padding=1, stride=1)\n",
    "\n",
    "        # Block 4:          256 x 4 x 4 --> 512 x 2 x 2\n",
    "        self.conv4 = torch.nn.Conv2d(256, 512, kernel_size=3, padding=1, stride=1)\n",
    "\n",
    "        # Linear layers:   512 x 2 x 2 --> 2048 --> 4096 --> 4096 --> 10\n",
    "        self.mp = torch.nn.MaxPool2d(2)\n",
    "\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(2048,4096)\n",
    "        self.fc2 = torch.nn.Linear(4096,4096)\n",
    "        self.fc3 = torch.nn.Linear(4096,10)\n",
    "\n",
    "\n",
    "        # self.logsoftmax = torch.nn.LogSoftmax()\n",
    "        ##############################################################################\n",
    "        #                             END OF YOUR CODE                               #\n",
    "        ##############################################################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        ##############################################################################\n",
    "        # TODO: Implement forward path turning an input image to class probability.  #\n",
    "        # For activation function, please use ReLU.                                  #\n",
    "        ##############################################################################\n",
    "        in_size = x.size(0)\n",
    "        \n",
    "        x = self.relu(self.mp(self.conv1(x)))\n",
    "        x = self.relu(self.mp(self.conv2(x)))\n",
    "        x = self.relu(self.mp(self.conv3(x)))\n",
    "        x = self.relu(self.mp(self.conv4(x)))\n",
    "        x = x.view(in_size, -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        # x = self.logsoftmax(x)\n",
    "\n",
    "\n",
    "        # Block 1:         3 x 32 x 32 --> 64 x 16 x 16\n",
    "\n",
    "        # Block 2:         64 x 16 x 16 --> 128 x 8 x 8\n",
    "\n",
    "        # Block 3:         128 x 8 x 8 --> 256 x 4 x 4\n",
    "\n",
    "        # Block 4:         256 x 4 x 4 --> 512 x 2 x 2\n",
    "\n",
    "        # Linear layers:   512 x 2 x 2 --> 2048 --> 4096 --> 4096 --> 10\n",
    "\n",
    "        ##############################################################################\n",
    "        #                             END OF YOUR CODE                               #\n",
    "        ##############################################################################\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = ConvNet()\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CT0GtJL3Ag-I"
   },
   "source": [
    "### Training the Model\n",
    "\n",
    "Some default settings for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "QTnHJ4N0HxeJ"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epochs = 20\n",
    "batch_size = 128\n",
    "learning_rate = 0.25\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# Define loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# Build data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "data_loaders = {\"train\": train_loader, \"test\": test_loader}\n",
    "dataset_sizes = {\"train\": train_size, \"test\": test_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yI90mCN-HxeK"
   },
   "source": [
    "Here, your job is to implement two functions for training and testing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "i8ERy2xSuygX"
   },
   "outputs": [],
   "source": [
    "def eval_on_test_set(model):\n",
    "    model.eval()\n",
    "    running_error = 0\n",
    "\n",
    "    for data in test_loader:\n",
    "        pass\n",
    "        ##############################################################################\n",
    "        # TODO: Implement the evaluation process on test set.                        #\n",
    "        ##############################################################################\n",
    "\n",
    "        # Load inputs and labels and deploy to running device\n",
    "        \n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward batch data through the net\n",
    "        predicts = model(inputs)\n",
    "        predicts_label = torch.max(predicts,1)[1]\n",
    "        \n",
    "        # Compute the error made on this batch and add it to the running error\n",
    "        running_error += sum(predicts_label != labels)\n",
    "\n",
    "        ##############################################################################\n",
    "        #                             END OF YOUR CODE                               #\n",
    "        ##############################################################################\n",
    "        \n",
    "    total_error = running_error / test_size\n",
    "    print('error rate on test set = {:.2f}%'.format(total_error * 100))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "iL3QYrp7u1V4"
   },
   "outputs": [],
   "source": [
    "def train_net(model):\n",
    "    start=time.time()\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # set the running quatities to zero at the beginning of the epoch\n",
    "        running_loss = 0\n",
    "        running_error = 0\n",
    "    \n",
    "        for data in train_loader:\n",
    "            pass\n",
    "            ##############################################################################\n",
    "            # TODO: Implement the training process.                                      #\n",
    "            ##############################################################################\n",
    "\n",
    "            # Load inputs and labels and deploy to running device\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Set the gradients to zeros\n",
    "            optimizer.zero_grad()            \n",
    "            # Forward the batch data through the net       \n",
    "            predicts = model(inputs)\n",
    "            predicts_label = torch.max(predicts,1)[1]\n",
    "            # Compute the average of the losses of the data points in the minibatch\n",
    "            loss = criterion(predicts, labels)\n",
    "            # print(loss)\n",
    "            # Backward pass to compute gradients\n",
    "            loss.backward()\n",
    "            # Do one step of stochastic gradient descent\n",
    "            optimizer.step()\n",
    "            # Add the loss of this batch to the running loss\n",
    "            running_loss += loss\n",
    "            # Compute the error made on this batch and add it to the running error\n",
    "            running_error += sum(predicts_label != labels)\n",
    "            ##############################################################################\n",
    "            #                             END OF YOUR CODE                               #\n",
    "            ##############################################################################\n",
    "            \n",
    "        # Compute stats for the full training set\n",
    "        total_loss = running_loss / train_size\n",
    "        total_error = running_error / train_size\n",
    "        elapsed = (time.time()-start) / 60\n",
    "        \n",
    "        print('epoch= {} \\t time= {:.2f} min \\t loss= {:.3f} \\t error= {:.2f}%'.format(epoch, elapsed, total_loss, total_error * 100))\n",
    "        eval_on_test_set(model) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_nXj3ZAyHxeO"
   },
   "source": [
    "Run `train_net` and start training. After training, your error on the testing set should be under 30% (if not, please look back and check your codes, there might be something wrong with the network architecture or training process). Once the training phrase is completed, save the trained model on your device so it can be directly loaded in the following tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BidU95GmHxeO",
    "outputId": "18f674c3-9a05-4e71-8f18-09a320f7c202"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 0 \t time= 0.23 min \t loss= 0.015 \t error= 72.98%\n",
      "error rate on test set = 57.60%\n",
      "epoch= 1 \t time= 0.50 min \t loss= 0.011 \t error= 51.67%\n",
      "error rate on test set = 49.35%\n",
      "epoch= 2 \t time= 0.77 min \t loss= 0.009 \t error= 38.96%\n",
      "error rate on test set = 37.86%\n",
      "epoch= 3 \t time= 1.05 min \t loss= 0.007 \t error= 29.48%\n",
      "error rate on test set = 29.68%\n",
      "epoch= 4 \t time= 1.32 min \t loss= 0.005 \t error= 23.24%\n",
      "error rate on test set = 29.17%\n",
      "epoch= 5 \t time= 1.58 min \t loss= 0.004 \t error= 17.75%\n",
      "error rate on test set = 29.07%\n",
      "epoch= 6 \t time= 1.85 min \t loss= 0.003 \t error= 13.57%\n",
      "error rate on test set = 26.25%\n",
      "epoch= 7 \t time= 2.12 min \t loss= 0.002 \t error= 9.65%\n",
      "error rate on test set = 24.62%\n",
      "epoch= 8 \t time= 2.39 min \t loss= 0.002 \t error= 6.71%\n",
      "error rate on test set = 24.62%\n",
      "epoch= 9 \t time= 2.66 min \t loss= 0.001 \t error= 4.84%\n",
      "error rate on test set = 26.17%\n",
      "epoch= 10 \t time= 2.93 min \t loss= 0.001 \t error= 3.50%\n",
      "error rate on test set = 23.16%\n",
      "epoch= 11 \t time= 3.20 min \t loss= 0.001 \t error= 3.09%\n",
      "error rate on test set = 23.98%\n",
      "epoch= 12 \t time= 3.47 min \t loss= 0.001 \t error= 3.06%\n",
      "error rate on test set = 28.25%\n",
      "epoch= 13 \t time= 3.74 min \t loss= 0.000 \t error= 1.96%\n",
      "error rate on test set = 24.07%\n",
      "epoch= 14 \t time= 4.01 min \t loss= 0.000 \t error= 1.65%\n",
      "error rate on test set = 24.03%\n",
      "epoch= 15 \t time= 4.28 min \t loss= 0.000 \t error= 1.72%\n",
      "error rate on test set = 23.05%\n",
      "epoch= 16 \t time= 4.55 min \t loss= 0.000 \t error= 1.16%\n",
      "error rate on test set = 23.49%\n",
      "epoch= 17 \t time= 4.82 min \t loss= 0.000 \t error= 1.04%\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "train_net(model)\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), './model_cnn.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D94cT95fDn4q"
   },
   "source": [
    "### Ploting the Results\n",
    "\n",
    "Now you have finished model training, let's randomly pick some test images, feed them into your model and see how they turn out. (There is nothing to implement in this section.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OAEMjcgax1cK",
    "outputId": "1f72a2f5-7989-4c13-c506-44ccdfc6cd2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (conv2): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (conv3): Conv2d(128, 256, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (conv4): Conv2d(256, 512, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (relu): ReLU()\n",
       "  (fc1): Linear(in_features=2048, out_features=4096, bias=True)\n",
       "  (fc2): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  (fc3): Linear(in_features=4096, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert os.path.exists('./model_cnn.pt'), 'train the model first'\n",
    "# Load the trained model\n",
    "model.load_state_dict(torch.load('./model_cnn.pt', map_location=torch.device('cpu')))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "BiWyCjxO4zqr",
    "outputId": "06994cc0-212b-4d78-c2e1-3472a35d6ea2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6929\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Choose a picture at random\n",
    "idx = randint(0, test_size-1)\n",
    "print(idx)\n",
    "im, label = test_set[idx]\n",
    "org_im = inverse_transform(im)\n",
    "\n",
    "# Send to device, rescale, and view as a batch of 1 \n",
    "im = im.to(device)\n",
    "im = im.view(1,3,32,32)\n",
    "\n",
    "# Feed it to the net and display the confidence scores\n",
    "scores = model(im) \n",
    "probs = F.softmax(scores, dim=1)\n",
    "show_prob_cifar(org_im, label, probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14NVWYk4HxeQ"
   },
   "source": [
    "### Refining the Model\n",
    "\n",
    "Although the vallina model has achieved fair classification results, the test error is still high remaining huge room for further improvement. There are lots of methods to refine your model, such as **modifying the network architecture** (e.g., making your network deeper), **optimizing the learning strategy** (e.g., optimizer, loss function), and **tuning the hyperparameters** (e.g., learning rate, training iterations), etc. Please try at least **TWO** different methods to improve your model's performance, and discuss why these changes can work. Design and conduct your own experiments using the coding cell below, and present your experimental results and analysis in a decent way (e.g., drawing some neat figures/tables can help to convey your thoughts effectively). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "XBAyX2WgHxeQ"
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: Conduct experiments to refine your model (e.g., modify the backbone, #\n",
    "# alter the learning strategy, and tuning the hyperparameters, etc.).        #\n",
    "##############################################################################\n",
    "\n",
    "# your code\n",
    "\n",
    "##############################################################################\n",
    "#                             END OF YOUR CODE                               #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jxcpniqTHxeQ"
   },
   "source": [
    "---\n",
    "\n",
    "**Write your observations and analysis in this Markdown cell:**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-dQFsHMr_8ol"
   },
   "source": [
    "## Task 2: Model Interpretation\n",
    "\n",
    "The model is doing well, eating images and predicting results; however, everything that happens inside is opaque and hard to explain. So, how can we interpret how CNN sees and understands when making a decision? \n",
    "\n",
    "In this section, we are going to explore Grad-CAM, a visual explanation algorithm that generates heatmaps indicating where the network is \"looking\" in the forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhqJq9z3HxeR"
   },
   "source": [
    "#### Grad-CAM\n",
    "\n",
    "Gradient-weighted Class Activation Mapping (Grad-CAM) [1], uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. An overview of the workflow of Grad-CAM is shown below. Please read the paper [1], understand the algorithm, and implement `generate_cam`.\n",
    "\n",
    "<img src=\"./figures/gradcam_network.png\" width=\"800\"/>\n",
    "\n",
    "[1] [Selvaraju, Ramprasaath R., et al. \"Grad-cam: Visual explanations from deep networks via gradient-based localization.\" ICCV 2017.](https://arxiv.org/abs/1610.02391)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "XGyTqReTHxeR"
   },
   "outputs": [],
   "source": [
    "gradients = []  # A gloabl variable used to save the gradient\n",
    "def generate_cam(model, input_image, target_layer='conv4a', target_class=None):\n",
    "    \"\"\"\n",
    "    A function to generate Grad-CAM of specific layer and class on an input image using given model.\n",
    "  \n",
    "    Inputs\n",
    "    - model: A PyTorch model.\n",
    "    - input_image: A PyTorch Tensor of shape (1, C, H, W).\n",
    "    - target_layer: A String indicating the name of targeted convolutional layer being visualized (e.g., 'conv4').\n",
    "                    By default, use the last conv layer of the model.\n",
    "    - target_class: An Integer indicating the lable of targeted class being visualized (e.g., 1). \n",
    "                    If None, use the predicted class as target class.\n",
    "    \n",
    "    Returns: A NumPy Array of shape (N, C, H, W) showing the intended heatmap.\n",
    "    \"\"\"\n",
    "\n",
    "    ##############################################################################\n",
    "    # TODO: Given an input image, generate its Grad-CAM on target conv layer     #\n",
    "    # using the backward gradients from a specific class.                        #\n",
    "    # 1. Forward the input image, when you also need to register the gradient    #\n",
    "    # hook so as to get the gradient in backward pass (hint: register_hook).     #\n",
    "    # 2. Backward pass with specified target class, and get gradients.           #\n",
    "    # 3. Average each gradient, multiply with its conv output, and sum together. #\n",
    "    ##############################################################################\n",
    "    \n",
    "    # your code\n",
    "    \n",
    "    ##############################################################################\n",
    "    #                             END OF YOUR CODE                               #\n",
    "    ##############################################################################\n",
    "\n",
    "    # Post processing\n",
    "    cam = np.maximum(cam, 0)\n",
    "    cam = (cam - np.min(cam)) / (np.max(cam) - np.min(cam))  # Normalize between 0-1\n",
    "    cam = np.uint8(cam * 255)  # Scale between 0-255 to visualize\n",
    "    cam = np.uint8(Image.fromarray(cam).resize((input_image.shape[2],\n",
    "                    input_image.shape[3]), Image.ANTIALIAS))\n",
    "\n",
    "    return cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 534
    },
    "id": "t7pL7bDLHxeR",
    "outputId": "24132410-8e9a-43fb-9236-73390d7ed16a"
   },
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-197924274789>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_axis_off\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mcam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_cam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mheatmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheatmap_on_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_colormap_on_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morg_im\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hsv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-27e76f02cd10>\u001b[0m in \u001b[0;36mgenerate_cam\u001b[0;34m(model, input_image, target_layer, target_class)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Post processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mcam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mcam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcam\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcam\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Normalize between 0-1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mcam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcam\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Scale between 0-255 to visualize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'cam' referenced before assignment"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = randint(0, test_size-1)\n",
    "im, label = test_set[idx]\n",
    "org_im = inverse_transform(im)\n",
    "\n",
    "im = im.to(device)\n",
    "im = im.view(1,3,32,32)\n",
    "\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "ax.imshow(org_im)\n",
    "ax.set_title('Original Image')\n",
    "ax.set_axis_off()\n",
    "\n",
    "cam = generate_cam(model, im)\n",
    "heatmap, heatmap_on_image = apply_colormap_on_image(org_im, cam, 'hsv')\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "ax.imshow(heatmap_on_image)\n",
    "ax.set_title('Grad-CAM')\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4yY8NLqHxeS"
   },
   "source": [
    "### Performing Visual Explanation\n",
    "\n",
    "Now you have successfully built a powerful tool (i.e., Grad-CAM) that can assist you to visualize and understand the CNN models. Try to make full use of `generate_cam` and design at least **TWO** experiments that can further explain how CNN sees and understands the images. For example, apply Grad-CAM on different layers and compare their differences. Please quantitatively show some evidence (e.g., plotting some examplar images clearly and elegantly) with necessary code snippets, write down your observations and briefly explain each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2EzJpHpFHxeS"
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: Write code snippets to present your experiments (hint: if you want   #\n",
    "# to plot multiple images in one single figure, plt.subplot should help).    #\n",
    "##############################################################################\n",
    "\n",
    "# your code\n",
    "\n",
    "##############################################################################\n",
    "#                             END OF YOUR CODE                               #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTeDHoUTHxeS"
   },
   "source": [
    "---\n",
    "\n",
    "**Write your observations and analysis in this Markdown cell:**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-HdsDUNRBZUO"
   },
   "source": [
    "## Task 3: Adversarial Attack\n",
    "\n",
    "After going through some testing results in Task 1, you might think in most cases the model performs pretty well, predicting the correct classes with high confidence, and rarely making mistakes. However, the network is not as strong as you think. The network can be vulnerable and easily fooled by simply adding some very small distributions on input images. In this section, we are going to attack our previously trained model by generating adversarial images that visually look alike but crush the model without striking a blowing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nivz5c0yHxeT"
   },
   "source": [
    "### Adversarial Examples\n",
    "\n",
    "Adversarial examples are specialised inputs created with the purpose of confusing a neural network, resulting in the misclassification of a given input. These examples are usually generated by adding imperceptible non-random perturbations to the image, which are indistinguishable to human eye, but can cause the network to fail to identify the contents of the image. There are many kinds of such attack strategies, here we focus on the **Fast Gradient Sign Method (FGSM)** [2]. FGSM is a white box attack where the attacker has complete access to the model being attacked. The method uses the gradients of the loss with respect to the input image to create a new image that maximises the loss. Below shows a famous exmaple taken from the paper [2], where the classification of image showing \"panda\" is changed to \"gibbon\" after adding an imperceptibly small vector: $\\text{sign}(\\nabla_xJ(\\theta, x, y))$.\n",
    "\n",
    "<img src=\"./figures/adversarial_example.png\" width=\"800\"/>\n",
    "\n",
    "Read the paper [2] for more details, and then implement the following function `generate_adv_image`, which generates \"fooling images\" of target class to attack your network. \n",
    "\n",
    "[2] [Goodfellow, Ian J., Jonathon Shlens, and Christian Szegedy. \"Explaining and harnessing adversarial examples.\" ICLR 2015.](https://arxiv.org/abs/1412.6572)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aOpmHPxNHxeT"
   },
   "outputs": [],
   "source": [
    "def generate_adv_image(model, inputs, labels, targets=None, eps=0.07):\n",
    "    \"\"\"\n",
    "    A function to generate adversarial examples.\n",
    "  \n",
    "    Inputs\n",
    "    - model: A PyTorch model.\n",
    "    - inputs: Input images, a PyTorch Tensor of shape (N, C, H, W).\n",
    "    - labels: Ground truth labels of input images, a PyTorch Tensor of shape (N,).\n",
    "    - targets: The target classes you want the model to misclassify, a PyTorch Tensor of shape (N,).\n",
    "    \n",
    "    Returns: The adversarial examples of given images.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    ##############################################################################\n",
    "    # TODO: Implement Fast Gradient Sign Method to generate an adversarial image #\n",
    "    # that fools the model to predict incorrect class.                           #   \n",
    "    # 1. Get the gradients of the loss w.r.t to the input image.                 #\n",
    "    # 2. Get the sign of the gradients to create the perturbation.               #\n",
    "    # 3. Add the perturbation to the input image.                                #\n",
    "    ##############################################################################\n",
    "    \n",
    "    # your code\n",
    "\n",
    "    ##############################################################################\n",
    "    #                             END OF YOUR CODE                               #\n",
    "    ##############################################################################\n",
    "\n",
    "    return adv_inputs.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Md-8lUVgHxeT"
   },
   "source": [
    "### Attacking the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrqTAy8EHxeT"
   },
   "source": [
    "Randomly pick up some images from test set, and attack the model using adversarial examples generated by `generate_adv_image`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D-N8N7zqHxeT"
   },
   "outputs": [],
   "source": [
    "idx = randint(0, len(test_set)-1)\n",
    "im, label = test_set[idx]\n",
    "org_im = inverse_transform(im)\n",
    "\n",
    "im = im.to(device)\n",
    "im = im.view(1,3,32,32)\n",
    "\n",
    "print('Before attack:')\n",
    "scores = model(im) \n",
    "probs = F.softmax(scores, dim=1)\n",
    "show_prob_cifar(org_im, label, probs)\n",
    "\n",
    "adv_im = generate_adv_image(model, im, label)\n",
    "\n",
    "print('After attack:')\n",
    "scores = model(adv_im) \n",
    "probs = F.softmax(scores, dim=1)\n",
    "show_prob_cifar(inverse_transform(adv_im[0]), label, probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxqhtP7AHxeU"
   },
   "source": [
    "Run evaluation on the testing set to see how the model performs on the generated adversarial examples, and compare with results before attacks. Try different parameters (e.g., `targets`, `eps`), describe your observations with a brief explanation in the below cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lM0jrq7pHxeU"
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: Evaluate error rate on adversarial images generated from testing set #\n",
    "# with different parameter settings.                                         #\n",
    "##############################################################################\n",
    "\n",
    "# your code\n",
    "\n",
    "##############################################################################\n",
    "#                             END OF YOUR CODE                               #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cG0_7YBJHxeU"
   },
   "source": [
    "---\n",
    "\n",
    "**Write your observations and analysis in this Markdown cell:**\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "EE5934_Project1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
